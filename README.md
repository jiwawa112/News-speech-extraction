# News-speech-extraction
新闻人物言论自动提取
项目主要展示部分：新闻人物言论提取
主要功能描述
新闻人物言论提取
+ (1)对新闻段落进行预处理：以句号“。”，问号“？”，感叹号“!”，为基准对句子进行分割获得片段。如果遇到(。”)的形式将切分点后移至(”)反引号。
+ (2)分析句子结构：获得单个句子后，依照预设的特征将原句进行分割，确定发言人所在句段以及言论所在句段。
+ (3)信息抽取：通过句法依存分析 分析句子结构。以“说”(或其他与'说'相似的预设动词)为界，在左边搜寻‘发言人’，在右搜寻‘言论’的起始点。
+ (4)言论合并：初步获得人物言论后，基于句子向量来分析是否应将该言论与其余言论合并。


+ 存在问题:
+ (1) 切分句子时，存在双引号之中存在多个句子的情况。而双引号之间的内容，一般为“发言人”的言论。
+ (2) 发言人所在句段经常同时存在“发言人”和“言论”。
+ (3) 分词工具对一些实体的提取并不准确，可能将"太阳报"分为"太阳"和"报"。如果组成“发言人”的词语较长，如“太阳报有关负责人”，那么提取出的“发言人”就不准确
+ (4) 实现句子相似度算法时，需要加载预先训练好的词向量模型，导致速度较慢 

**哈工大语言模型安装：**尝试用python3.7版本安装未成功，后来发现平台最高支持3.6版本。
安装过程：https://pyltp.readthedocs.io/zh_CN/latest

**哈工大与jieba分词进行比较：对两个工具交叉功能进行比较，发现pyltp分词效果好于jieba，但缺点是执行效率慢。
**算法模型执行速度慢：实现句子相似度算法时，需要加载预先训练好的词向量模型。同时，发现哈工大的分词工具pyltp.segment执行每次在3秒以上，相当耗时。
**普林斯顿语句近似度算法：按照论文算法只完成了Vs=1/|s|*∑a/(a+p(w))*Vw，后半部分的矩阵运算不知道该用如何工具去运算。
